{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNp2v/3zudZ+oY8Qovn3X9Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gajendra-theDataEngineer004627/BigDataAssignments/blob/Assignment11/Assignment12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyRO9ecU-ukY"
      },
      "outputs": [],
      "source": [
        "!apt-get update -y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "metadata": {
        "id": "X9lj_grK_yZ7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz"
      ],
      "metadata": {
        "id": "jIRloHHH_4fb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xf spark-3.1.2-bin-hadoop2.7.tgz"
      ],
      "metadata": {
        "id": "V91ktK6e_6AL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop2.7\""
      ],
      "metadata": {
        "id": "HgzeDwYc_mCQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "M4GzGUoUAH4W"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = (SparkSession\n",
        " .builder\n",
        " .appName(\"<app_name>\")\n",
        " .getOrCreate())"
      ],
      "metadata": {
        "id": "pgzOWk8_--Uw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to doanload data files from the google drive\n",
        "import gdown\n",
        "def downloadfiles_fromFolder() :\n",
        "    url = \"https://drive.google.com/drive/folders/1Hf8PijpBSNyDjwr-YgozGDRL8UqMDq3D?usp=sharing\"   # can also be used input()\n",
        "    if url.split(\"/\")[-1]== \"?usp=sharing\":\n",
        "       url = url.replace(\"?usp=sharing\",\"\")\n",
        "    gdown.download_folder(url, output=\"/content\")\n",
        "\n",
        "downloadfiles_fromFolder()"
      ],
      "metadata": {
        "id": "Zm1ANzlU_CQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the data in dataframe and then validate accordingly\n",
        "\n",
        "deptDf = spark.read.format(\"json\").option(\"path\",\"/content/dept.json\").load()\n",
        "empDf = spark.read.format(\"json\").option(\"path\",\"/content/employee.json\").load()\n",
        "\n",
        "deptDf1 = deptDf.withColumnRenamed(\"deptid\",\"deptid_dept\")\n",
        "empDf1 = empDf.withColumnRenamed(\"deptid\",\"deptid_emp\")\n",
        "deptDf1.show()\n",
        "empDf1.show(truncate =False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXDQ6iPeQJL4",
        "outputId": "86579209-7c80-432b-e20f-ab146a11becc"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----------+\n",
            "| deptName|deptid_dept|\n",
            "+---------+-----------+\n",
            "|       IT|         11|\n",
            "|       HR|         21|\n",
            "|Marketing|         31|\n",
            "|      Fin|         41|\n",
            "|    Admin|         51|\n",
            "+---------+-----------+\n",
            "\n",
            "+---------------------+---+----------+-------+----+------+\n",
            "|address              |age|deptid_emp|empname|id  |salary|\n",
            "+---------------------+---+----------+-------+----+------+\n",
            "|[{Pune, Maharashtra}]|25 |11        |satish |1201|5000  |\n",
            "|[{Pune, Maharashtra}]|28 |21        |krishna|1202|6000  |\n",
            "|[{Pune, Maharashtra}]|39 |31        |amith  |1203|7000  |\n",
            "|[{Pune, Maharashtra}]|23 |41        |javed  |1204|8000  |\n",
            "|[{Pune, Maharashtra}]|23 |41        |prudvi |1205|9000  |\n",
            "+---------------------+---+----------+-------+----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Problem 1: Given 2 Datasets employee.json and dept.json\n",
        "We need to calculate the count of employees against each department. Use\n",
        "Structured APIâ€™s.   header : depName,deptid,empcount\"\"\"\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "join_condn = deptDf1.deptid_dept == empDf1.deptid_emp\n",
        "joinedDf = deptDf1.join(empDf1,join_condn,\"LEFT\")\n",
        "joinedDf.show(truncate = False)\n",
        "\n",
        "result = joinedDf.select(\n",
        "    F.col(\"deptName\").alias(\"deptName\"),\n",
        "    F.col(\"deptid_dept\").alias(\"deptid\"),\n",
        "    F.count(\"id\").over(Window.partitionBy(\"deptid_dept\").orderBy(\"deptid_dept\")).alias(\"empCount\")\n",
        ").distinct().orderBy(\"deptid_dept\")\n",
        "\n",
        "\n",
        "result1 = joinedDf.selectExpr(\"deptName\",\"deptid_dept as deptid\",\n",
        "                              \"count(id)over(partition by deptid_dept order by deptid_dept) as empCount1\").distinct().orderBy(\"deptid_dept\")\n",
        "\n",
        "# worked on both ways column object and using Sql Expression  final results\n",
        "result.show()\n",
        "result1.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mjQ66rfRjBf",
        "outputId": "21250ac9-40b8-480f-8075-475d81c4b665"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----------+---------------------+----+----------+-------+----+------+\n",
            "|deptName |deptid_dept|address              |age |deptid_emp|empname|id  |salary|\n",
            "+---------+-----------+---------------------+----+----------+-------+----+------+\n",
            "|IT       |11         |[{Pune, Maharashtra}]|25  |11        |satish |1201|5000  |\n",
            "|HR       |21         |[{Pune, Maharashtra}]|28  |21        |krishna|1202|6000  |\n",
            "|Marketing|31         |[{Pune, Maharashtra}]|39  |31        |amith  |1203|7000  |\n",
            "|Fin      |41         |[{Pune, Maharashtra}]|23  |41        |prudvi |1205|9000  |\n",
            "|Fin      |41         |[{Pune, Maharashtra}]|23  |41        |javed  |1204|8000  |\n",
            "|Admin    |51         |null                 |null|null      |null   |null|null  |\n",
            "+---------+-----------+---------------------+----+----------+-------+----+------+\n",
            "\n",
            "+---------+------+--------+\n",
            "| deptName|deptid|empCount|\n",
            "+---------+------+--------+\n",
            "|       IT|    11|       1|\n",
            "|       HR|    21|       1|\n",
            "|Marketing|    31|       1|\n",
            "|      Fin|    41|       2|\n",
            "|    Admin|    51|       0|\n",
            "+---------+------+--------+\n",
            "\n",
            "+---------+------+---------+\n",
            "| deptName|deptid|empCount1|\n",
            "+---------+------+---------+\n",
            "|       IT|    11|        1|\n",
            "|       HR|    21|        1|\n",
            "|Marketing|    31|        1|\n",
            "|      Fin|    41|        2|\n",
            "|    Admin|    51|        0|\n",
            "+---------+------+---------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}