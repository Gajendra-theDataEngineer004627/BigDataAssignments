{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPuXNMbr6EOHFVSont02rDC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gajendra-theDataEngineer004627/BigDataAssignments/blob/Assignment11/Assignment12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kyRO9ecU-ukY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "860cc0ad-770e-4c79-e2df-3a0cac155eee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.36)] [\r                                                                               \rGet:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "\r0% [2 InRelease 12.7 kB/119 kB 11%] [Waiting for headers] [Waiting for headers]\r                                                                               \rGet:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\r0% [2 InRelease 21.4 kB/119 kB 18%] [Waiting for headers] [3 InRelease 0 B/3,62\r0% [2 InRelease 21.4 kB/119 kB 18%] [Waiting for headers] [Connected to ppa.lau\r                                                                               \rGet:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "\r0% [2 InRelease 47.5 kB/119 kB 40%] [Waiting for headers] [Connected to ppa.lau\r                                                                               \rGet:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [515 kB]\n",
            "Get:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,283 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,260 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1,158 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [49.9 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [50.4 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [28.1 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [998 kB]\n",
            "Hit:17 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1,137 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [44.0 kB]\n",
            "Hit:20 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:21 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [21.8 kB]\n",
            "Fetched 6,906 kB in 3s (2,617 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ],
      "source": [
        "!apt-get update -y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "metadata": {
        "id": "X9lj_grK_yZ7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz"
      ],
      "metadata": {
        "id": "jIRloHHH_4fb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xf spark-3.1.2-bin-hadoop2.7.tgz"
      ],
      "metadata": {
        "id": "V91ktK6e_6AL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop2.7\""
      ],
      "metadata": {
        "id": "HgzeDwYc_mCQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "M4GzGUoUAH4W"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = (SparkSession\n",
        " .builder\n",
        " .appName(\"<app_name>\")\n",
        " .getOrCreate())"
      ],
      "metadata": {
        "id": "pgzOWk8_--Uw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to doanload data files from the google drive\n",
        "import gdown\n",
        "def downloadfiles_fromFolder() :\n",
        "    url = \"https://drive.google.com/drive/folders/1Hf8PijpBSNyDjwr-YgozGDRL8UqMDq3D?usp=sharing\"   # can also be used input()\n",
        "    if url.split(\"/\")[-1]== \"?usp=sharing\":\n",
        "       url = url.replace(\"?usp=sharing\",\"\")\n",
        "    gdown.download_folder(url, output=\"/content\")\n",
        "\n",
        "downloadfiles_fromFolder()"
      ],
      "metadata": {
        "id": "Zm1ANzlU_CQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the data in dataframe and then validate accordingly\n",
        "\n",
        "deptDf = spark.read.format(\"json\").option(\"path\",\"/content/dept.json\").load()\n",
        "empDf = spark.read.format(\"json\").option(\"path\",\"/content/employee.json\").load()\n",
        "\n",
        "# renamed the ambiguous columns for future usage\n",
        "deptDf1 = deptDf.withColumnRenamed(\"deptid\",\"deptid_dept\")\n",
        "empDf1 = empDf.withColumnRenamed(\"deptid\",\"deptid_emp\")\n",
        "deptDf1.show()\n",
        "empDf1.show(truncate =False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXDQ6iPeQJL4",
        "outputId": "ba38bbb7-1849-47f6-de14-c9c3849f57e9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----------+\n",
            "| deptName|deptid_dept|\n",
            "+---------+-----------+\n",
            "|       IT|         11|\n",
            "|       HR|         21|\n",
            "|Marketing|         31|\n",
            "|      Fin|         41|\n",
            "|    Admin|         51|\n",
            "+---------+-----------+\n",
            "\n",
            "+---------------------+---+----------+-------+----+------+\n",
            "|address              |age|deptid_emp|empname|id  |salary|\n",
            "+---------------------+---+----------+-------+----+------+\n",
            "|[{Pune, Maharashtra}]|25 |11        |satish |1201|5000  |\n",
            "|[{Pune, Maharashtra}]|28 |21        |krishna|1202|6000  |\n",
            "|[{Pune, Maharashtra}]|39 |31        |amith  |1203|7000  |\n",
            "|[{Pune, Maharashtra}]|23 |41        |javed  |1204|8000  |\n",
            "|[{Pune, Maharashtra}]|23 |41        |prudvi |1205|9000  |\n",
            "+---------------------+---+----------+-------+----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Problem 1: Given 2 Datasets employee.json and dept.json\n",
        "We need to calculate the count of employees against each department. Use\n",
        "Structured APIâ€™s.   header : depName,deptid,empcount\"\"\"\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "join_condn = deptDf1.deptid_dept == empDf1.deptid_emp\n",
        "joinedDf = deptDf1.join(empDf1,join_condn,\"LEFT\")\n",
        "joinedDf.show(truncate = False)\n",
        "\n",
        "result = joinedDf.select(\n",
        "    F.col(\"deptName\").alias(\"deptName\"),\n",
        "    F.col(\"deptid_dept\").alias(\"deptid\"),\n",
        "    F.count(\"id\").over(Window.partitionBy(\"deptid_dept\").orderBy(\"deptid_dept\")).alias(\"empCount\")\n",
        ").distinct().orderBy(\"deptid_dept\")\n",
        "\n",
        "\n",
        "result1 = joinedDf.selectExpr(\"deptName\",\"deptid_dept as deptid\",\n",
        "                              \"count(id)over(partition by deptid_dept order by deptid_dept) as empCount1\").distinct().orderBy(\"deptid_dept\")\n",
        "\n",
        "# worked on both ways column object and using Sql Expression  final results\n",
        "result.show()\n",
        "result1.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mjQ66rfRjBf",
        "outputId": "458a6b58-b3dc-451e-9426-8308482c3027"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----------+---------------------+----+----------+-------+----+------+\n",
            "|deptName |deptid_dept|address              |age |deptid_emp|empname|id  |salary|\n",
            "+---------+-----------+---------------------+----+----------+-------+----+------+\n",
            "|IT       |11         |[{Pune, Maharashtra}]|25  |11        |satish |1201|5000  |\n",
            "|HR       |21         |[{Pune, Maharashtra}]|28  |21        |krishna|1202|6000  |\n",
            "|Marketing|31         |[{Pune, Maharashtra}]|39  |31        |amith  |1203|7000  |\n",
            "|Fin      |41         |[{Pune, Maharashtra}]|23  |41        |prudvi |1205|9000  |\n",
            "|Fin      |41         |[{Pune, Maharashtra}]|23  |41        |javed  |1204|8000  |\n",
            "|Admin    |51         |null                 |null|null      |null   |null|null  |\n",
            "+---------+-----------+---------------------+----+----------+-------+----+------+\n",
            "\n",
            "+---------+------+--------+\n",
            "| deptName|deptid|empCount|\n",
            "+---------+------+--------+\n",
            "|       IT|    11|       1|\n",
            "|       HR|    21|       1|\n",
            "|Marketing|    31|       1|\n",
            "|      Fin|    41|       2|\n",
            "|    Admin|    51|       0|\n",
            "+---------+------+--------+\n",
            "\n",
            "+---------+------+---------+\n",
            "| deptName|deptid|empCount1|\n",
            "+---------+------+---------+\n",
            "|       IT|    11|        1|\n",
            "|       HR|    21|        1|\n",
            "|Marketing|    31|        1|\n",
            "|      Fin|    41|        2|\n",
            "|    Admin|    51|        0|\n",
            "+---------+------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Problem 3:\n",
        "File A is a text file of size 1.2 GB in HDFS at location /loc/x. It contains match by\n",
        "match statistics of runs scored by all the batsman in the history of cricket.\n",
        "File B is a text file of size 1.2 MB present in local dir /loc/y. It contains list of\n",
        "batsman playing in cricket world cup 2019.\n",
        "\n",
        "Find the batsman participating in 2019 who has the best average of\n",
        "scoring runs in his career. Solve using Dataframe or datasets.\n",
        "\n",
        "\"\"\"\n",
        "# as we can use data loading fields as list or we can use direct files\n",
        "\"\"\"\n",
        "list1 = [(1,\"Rohit Sharma\",\"India\",200,100.2),\n",
        "(1,\"Virat Kohli\",\"India\",100,98.02),\n",
        "(1,\"Steven Smith\",\"Aus\",77,79.23),\n",
        "(35,\"Clive Lloyd\",\"WI\",29,37.00),\n",
        "(243,\"Rohit Sharma\",\"India\",23,150.00),\n",
        "(243,\"Faf du Plesis\",\"SA\",17,35.06)]\n",
        "\n",
        "list2 = [(\"Rohit_Sharma\",\"India\"),\n",
        "(\"Steven_Smith\",\"Aus\"),\n",
        "(\"Virat_Kohli\",\"India\")]\n",
        "\"\"\"\n",
        "fileA = spark.read.format(\"csv\").option(\"header\",True)\\\n",
        ".option(\"inferSchema\", True).option(\"path\",\"/content/fileA.csv\").load()\n",
        "\n",
        "fileB = spark.read.format(\"csv\").option(\"header\",True)\\\n",
        ".option(\"inferSchema\", True).option(\"path\",\"/content/fileB.csv\").load()\n",
        "\n",
        "fileA1 = fileA.withColumnRenamed(\"Batsman\", \"Batsman_A\")\n",
        "fileB1 = fileB.withColumnRenamed(\"Batsman\", \"Batsman_B\")\n",
        "# autoBroadcast Join is turned off\n",
        "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
        "#fileA.show()\n",
        "#fileB.show()\n",
        "\n",
        "# the best average of scoring runs in his career\n",
        "# using broadcast join\n",
        "from pyspark.sql.functions import broadcast\n",
        "from pyspark.sql.functions import desc\n",
        "\n",
        "joining_condition = fileA1.Batsman_A == fileB1.Batsman_B\n",
        "joinedAvgDf = fileA1.join(broadcast(fileB1), joining_condition, \"INNER\" )\n",
        "joinedAvgDf.show()\n",
        "\n",
        "resultDf = joinedAvgDf.selectExpr(\"Batsman_A as Batsman\",\n",
        "\"ROUND(((sum(RunsScored)over(partition by Batsman_A order by Batsman_A))*1.0/count(Batsman_A)over(partition by Batsman_A order by Batsman_A)), 2) as AvgScore\")\\\n",
        ".distinct().orderBy(desc(\"AvgScore\"))\n",
        "\n",
        "# ----Final Result ---- #\n",
        "\n",
        "resultDf.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSd3Z11PeIw1",
        "outputId": "ce270e23-d8cd-4498-9439-1955f9ed0770"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------+-----+----------+----------+------------+-----+\n",
            "|MatchNumber|   Batsman_A| Team|RunsScored|StrikeRate|   Batsman_B| Team|\n",
            "+-----------+------------+-----+----------+----------+------------+-----+\n",
            "|          1|Rohit Sharma|India|       200|     100.2|Rohit Sharma|India|\n",
            "|          1| Virat Kohli|India|       100|     98.02| Virat Kohli|India|\n",
            "|          1|Steven Smith|  Aus|        77|     79.23|Steven Smith|  Aus|\n",
            "|        243|Rohit Sharma|India|        23|     150.0|Rohit Sharma|India|\n",
            "+-----------+------------+-----+----------+----------+------------+-----+\n",
            "\n",
            "+------------+--------+\n",
            "|     Batsman|AvgScore|\n",
            "+------------+--------+\n",
            "|Rohit Sharma|  111.50|\n",
            "| Virat Kohli|  100.00|\n",
            "|Steven Smith|   77.00|\n",
            "+------------+--------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}